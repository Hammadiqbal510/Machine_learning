{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_algorithm.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN/SPmazhXoRSL7LW7BMG45",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hammadiqbal510/Machine_learning/blob/master/tensorflow_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZGxl98cIhv_"
      },
      "source": [
        "pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX9MUZgYJfA3"
      },
      "source": [
        "pip install tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSE-WUqIKx0G"
      },
      "source": [
        "%tensorflow_version "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMPI8_2xLVui"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGBTIWh3LnPK"
      },
      "source": [
        "%tensorflow_version \n",
        "import tensorflow as tf\n",
        "print(tf.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbeny3biM7Zn"
      },
      "source": [
        "creat a tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuKHOfckM5LT"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYf0QUSXM_az"
      },
      "source": [
        "import tensorflow as tf\n",
        "string=tf.Variable(\"hammad\",tf.string)\n",
        "integer=tf.Variable(32,tf.int64)\n",
        "floating=tf.Variable(23.23,tf.float64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF2mJJ1AOg_K"
      },
      "source": [
        "Rank/degree of Tensorflow\n",
        "\n",
        "\n",
        "number of dimension involved in tensor \n",
        "atleast have one 1 value if its have no value 0 zero is known as scaler\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTvdKEIONqF6"
      },
      "source": [
        "rank1_tensor=tf.Variable([\"hammad\",\"hammad1\"],tf.string) #this is one 1 dimesion rank of tensor bcz there is one list\n",
        "rank2_tensor=tf.Variable([[\"hammad\",\"hammad1\",\"hammad\"],[\"iqbal\",\"iqbal\",\"iqbal\"]],tf.string) #this is two 2 dimesion rank of tensor bcz there is  list in a list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SStupX6nQbWR"
      },
      "source": [
        "Rank=tf.rank(rank2_tensor)\n",
        "Rank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUlbeQjDQxFd"
      },
      "source": [
        "#examples:\n",
        "tf.rank(string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSclT1QXRpAP"
      },
      "source": [
        "tf.rank(integer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TGhzNpZSCv7"
      },
      "source": [
        "tf.rank(floating)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXYIJWeRSFVK"
      },
      "source": [
        "tf.rank(rank1_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mo_borwSLp8"
      },
      "source": [
        "tf.rank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvU1tErpSOAX"
      },
      "source": [
        "_,tf.rank  # _ is use for get all variable which is define"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMi8Lsl7SnRA"
      },
      "source": [
        "shape of the tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrpA7ywJSTMn"
      },
      "source": [
        "rank1_tensor.shape # simply have a list and have 2 elemnts in list thats show 2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_L4ODFWSsmR"
      },
      "source": [
        "rank2_tensor.shape #list in a list and have 3 elemnts in each list thats show 2  and other have 3 elemnts in list thats show 3\n",
        "  #note that both list have equal elemnts bcz it s list in a list\n",
        "\n",
        "  # [2,3]. 2 show there is 2 lists and 3 show no. of elemnts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guapHWlnUmL5"
      },
      "source": [
        "Reshape of the Tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl29QQADSvqN"
      },
      "source": [
        "tensor_1=tf.ones([1,2,3])\n",
        "tensor_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWvApzE1U5Zu"
      },
      "source": [
        "tensor_1= tf.ones([1,2,3])\n",
        "tensor_2= tf.reshape(tensor_1, [2,3,1])  # oppposite or same or in range these value is those type here which is  define in tf.one other wise it genrate erroe\n",
        "print(tensor_2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwbCUuPDVUdV"
      },
      "source": [
        "tensor_1= tf.ones([1,2,3])\n",
        "tensor_2= tf.reshape(tensor_1, [2,3,1]) \n",
        " # oppposite or same or in range these value is those type here which is  define in tf.one other wise it genrate erroe\n",
        "tensor_3=tf.reshape(tensor_2,[3,-1])\n",
        "print(tensor_1) \n",
        "print(tensor_2) \n",
        "print(tensor_3) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn_-jGjt12q2"
      },
      "source": [
        "types of tensor:  (all the tensor are immutable)\n",
        "\n",
        "variable \n",
        "\n",
        "constant\n",
        "\n",
        "placeholder\n",
        "\n",
        "sparsetensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVHKcGAU16eF"
      },
      "source": [
        "import tensorflow as tf\n",
        "table=tf.zeros([5,5,5,5])\n",
        "print(table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLfK342S33Bf"
      },
      "source": [
        "after this we can reshape it\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNFa6Bup37Ax"
      },
      "source": [
        "table_2=tf.reshape(table,[625])\n",
        "print(table_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J4Cx9xf4BuO"
      },
      "source": [
        "table_3=tf.reshape(table,[125,-1])\n",
        "print(table_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV4UihvaTO29"
      },
      "source": [
        "!pip install -q sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHzozL6pWlwf"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from __future__ import  division,absolute_import,print_function,unicode_literals\n",
        "import numpy as np #use for solve multi dimension calculation\n",
        "import pandas as pd #muniplate and visualize the data sets\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt #visualization of graph and charts\n",
        "from six.moves import urllib\n",
        "import tensorflow.compat.v2.feature_column as fc\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUGfH_oqW0bb"
      },
      "source": [
        "train_data= pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
        "test_data = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
        "result=train_data.head()\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbpQr5kudV3h"
      },
      "source": [
        "train_data= pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
        "test_data = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
        "train=train_data.pop(\"survived\")\n",
        "test=test_data.pop(\"survived\")\n",
        "result=train_data.head()\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AMBxnsJdu-s"
      },
      "source": [
        "train_data= pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
        "test_data = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
        "train=train_data.pop(\"survived\")\n",
        "test=test_data.pop(\"survived\")\n",
        "result=train_data.head()\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf9ROdndeHJ_"
      },
      "source": [
        "train_data= pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
        "test_data = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
        "train=train_data.pop(\"survived\")\n",
        "test=test_data.pop(\"survived\")\n",
        "print(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xblK05tMejx9"
      },
      "source": [
        "train_data= pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
        "test_data = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
        "train=train_data.pop(\"survived\")\n",
        "test=test_data.pop(\"survived\")\n",
        "print(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIQk_gU9e2oq"
      },
      "source": [
        "if u want to get specific column from data set...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZPR2cpWenri"
      },
      "source": [
        "train_data= pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
        "test_data = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
        "train=train_data.pop(\"survived\")\n",
        "test=test_data.pop(\"survived\")\n",
        "print(train_data[\"sex\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQP2eMCmfKbw"
      },
      "source": [
        "train_data= pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
        "test_data = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
        "train=train_data.pop(\"survived\")\n",
        "test=test_data.pop(\"survived\")\n",
        "print(train_data[\"age\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMiARdZBfg1x"
      },
      "source": [
        "if u want to get value of first person in colum by index-wise..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxWtYr3jfPkz"
      },
      "source": [
        "train_data= pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
        "test_data = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
        "train=train_data.pop(\"survived\")\n",
        "test=test_data.pop(\"survived\")\n",
        "print(train_data.loc[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv9bMyG5fxm-"
      },
      "source": [
        "train_data= pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
        "test_data = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
        "train=train_data.pop(\"survived\")\n",
        "test=test_data.pop(\"survived\")\n",
        "print(train_data.loc[1],test_data.loc[3])   #we get all the values by index wise thats phla bnda ki age name sex kia haa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yomy5AYGgM9B"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTSvY8FwhZQU"
      },
      "source": [
        "\n",
        "And if we want a more statistical analysis of our data we can use the .describe() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcjHGty0gpoE"
      },
      "source": [
        "test_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJqVzzVvkTPl"
      },
      "source": [
        "train_data.age.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tbOzVocgtuw"
      },
      "source": [
        "train_data.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6qGTnPohfKY"
      },
      "source": [
        "train_data.shape #get the shape ( row, coumn ) 0f data sets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnmgY5HoAfgX"
      },
      "source": [
        "train_data[\"age\"].unique() #to get unique value of  desire column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bywy30m0AqJi"
      },
      "source": [
        "train_data[\"sex\"].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBSv3dXzhmiw"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EivybsNWiDpi"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQnmoTKsiJj5"
      },
      "source": [
        "train_data.sex.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THsVK_iWibbJ"
      },
      "source": [
        "train_data.sex.hist(bins=20) #Bins are the number of intervals you want to divide all of your data into,\n",
        "#such that it can be displayed as bars on a histogram. A simple method to work our how many bins are suitable is to take the square root of the total number of values in your distribution."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAa8OrKcmG_u"
      },
      "source": [
        "similarly ther sre many type or kind of graph which we show "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6RyvcLrihaC"
      },
      "source": [
        "train_data.age.hist(bins=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5mOU85QjNsD"
      },
      "source": [
        "train_data.sex.value_counts().plot(kind=\"barh\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV6Se_FVjfIL"
      },
      "source": [
        "train_data[\"class\"].value_counts().plot(kind=\"barh\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOo9F0W1lEOA"
      },
      "source": [
        "train_data.age.value_counts().plot(kind=\"pie\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzh63kIHlqO9"
      },
      "source": [
        "train_data.sex.value_counts().plot(kind=\"line\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElZurTCj4eTJ"
      },
      "source": [
        "pd.concat([train_data,train],axis=1).groupby(\"sex\").survived.mean().plot(kind=\"pie\").set_xlabel(\"%survived\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enZCqqJO6NsI"
      },
      "source": [
        "pd.concat([train_data,train], axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('% survive')\n",
        "#set_xlabel is used to set label on x axis\n",
        "#axis=1 are also use in  x ,y axis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vAFZmFn96sy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l42qmk_bVHvD"
      },
      "source": [
        "After analyzing this information, we should notice the following:\n",
        "- Most passengers are in their 20's or 30's \n",
        "- Most passengers are male\n",
        "- Most passengers are in \"Third\" class\n",
        "- Females have a much higher chance of survival\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2CU7pAl7Dgd"
      },
      "source": [
        "train_data= pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
        "test_data = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
        "train=train_data.pop(\"survived\")\n",
        "test=test_data.pop(\"survived\")\n",
        "CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\n",
        "                       'embark_town', 'alone']\n",
        "NUMERIC_COLUMNS = ['age', 'fare']\n",
        "\n",
        "feature_columns = []\n",
        "for feature_name in CATEGORICAL_COLUMNS:\n",
        "  vocabulary = train_data[feature_name].unique()  # gets a list of all unique values from given feature column\n",
        "  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n",
        "\n",
        "for feature_name in NUMERIC_COLUMNS:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
        "\n",
        "print(feature_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJw64dUoDvuL"
      },
      "source": [
        "train_data[\"n_siblings_spouses\"].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGGaMi7zCFLu"
      },
      "source": [
        "The Training Process\n",
        "So, we are almost done preparing our dataset and I feel as though it's a good time to explain how our model is trained. Specifically, how input data is fed to our model.\n",
        "\n",
        "For this specific model data is going to be streamed into it in small batches of 32. This means we will not feed the entire dataset to our model at once, but simply small batches of entries. We will feed these batches to our model multiple times according to the number of epochs.\n",
        "\n",
        "An epoch is simply one stream of our entire dataset. The number of epochs we define is the amount of times our model will see the entire dataset. We use multiple epochs in hope that after seeing the same data multiple times the model will better determine how to estimate it.\n",
        "\n",
        "Ex. if we have 10 ephocs, our model will see the same dataset 10 times.\n",
        "\n",
        "Since we need to feed our data in batches and multiple times, we need to create something called an input function. The input function simply defines how our dataset will be converted into batches at each epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUfA9tBuNBdH"
      },
      "source": [
        "**Input Function**\n",
        "\n",
        "\n",
        "The TensorFlow model we are going to use requires that the data we pass it comes in as a tf.data.Dataset object. This means we must create a input function that can convert our current pandas dataframe into that object.\n",
        "\n",
        "Below you'll see a seemingly complicated input function, this is straight from the TensorFlow documentation (https://www.tensorflow.org/tutorials/estimator/linear). I've commented as much as I can to make it understandble, but you may want to refer to the documentation for a detailed explination of each method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVQ8AYOk4o56"
      },
      "source": [
        "# The input function simply defines how our dataset will be converted into batches at each epoch.\n",
        "def make_input_function(data,label,batch_size=32,epoch=10,shuffle=True):\n",
        "  def input_function(): # inner function, this will be returned\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(data), label)) # create tf.data.Dataset object with data and its label\n",
        "    if shuffle:\n",
        "      ds=ds.shuffle(1000)\n",
        "    ds=ds.batch(batch_size).repeat(epoch) # split dataset into batches of 32 and repeat process for number of epochs\n",
        "    return ds # return a batch of the dataset\n",
        "  return input_function\n",
        "train_input_function=make_input_function(train_data,train)\n",
        "test_input_function=make_input_function(test_data,test,epoch=1, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6rhqsZjDkC7"
      },
      "source": [
        "**Creating the Model**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In this tutorial we are going to use a linear estimator to utilize the linear regression algorithm.\n",
        "\n",
        "Creating one is pretty easy! Have a look below.\n",
        "** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-pz1JKAL_nX"
      },
      "source": [
        "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)\n",
        "# We create a linear estimtor by passing the feature columns we created earlier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evLWQ-TeFXTS"
      },
      "source": [
        "**Training the Model**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Training the model is as easy as passing the input functions that we created earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JXk1Vu2NIPG"
      },
      "source": [
        "The Training Process\n",
        "So, we are almost done preparing our dataset and I feel as though it's a good time to explain how our model is trained. Specifically, how input data is fed to our model.\n",
        "\n",
        "For this specific model data is going to be streamed into it in small batches of 32. This means we will not feed the entire dataset to our model at once, but simply small batches of entries. We will feed these batches to our model multiple times according to the number of epochs.\n",
        "\n",
        "An epoch is simply one stream of our entire dataset. The number of epochs we define is the amount of times our model will see the entire dataset. We use multiple epochs in hope that after seeing the same data multiple times the model will better determine how to estimate it.\n",
        "\n",
        "Ex. if we have 10 ephocs, our model will see the same dataset 10 times.\n",
        "\n",
        "Since we need to feed our data in batches and multiple times, we need to create something called an input function. The input function simply defines how our dataset will be converted into batches at each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdiZdIyaEqgE"
      },
      "source": [
        "#Training the Model\n",
        "\n",
        "#Training the model is as easy as passing the input functions that we created earlier.\n",
        "linear_est.train(train_input_function)   # train\n",
        "result = linear_est.evaluate(test_input_function)  # get model metrics/stats by testing on tetsing data\n",
        "\n",
        "clear_output()  # clears consoke output\n",
        "print(result['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xCv0YBfMMRG"
      },
      "source": [
        "**NOTE**\n",
        "\n",
        "**whole code is run here where we create model and train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7WSpfqjHY5s"
      },
      "source": [
        "#uper wala function same neacha lkha dana ha uper hun na function bnaya nacha hum model create kar raha r us ko train kar rahaa\n",
        " # **as we know upper row same input function define here**\n",
        "# The input function simply defines how our dataset will be converted into batches at each epoch.\n",
        "def make_input_function(data,label,batch_size=32,epoch=10,shuffle=True):\n",
        "  def input_function(): # inner function, this will be returned\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(data), label)) # create tf.data.Dataset object with data and its label\n",
        "    if shuffle:\n",
        "      ds=ds.shuffle(1000)\n",
        "    ds=ds.batch(batch_size).repeat(epoch) # split dataset into batches of 32 and repeat process for number of epochs\n",
        "    return ds # return a batch of the dataset\n",
        "  return input_function\n",
        "train_input_function=make_input_function(train_data,train)\n",
        "test_input_function=make_input_function(test_data,test,epoch=1, shuffle=False)\n",
        "\n",
        "\n",
        "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)\n",
        "# We create a linear estimtor by passing the feature columns we created earlier\n",
        "\n",
        "\n",
        "\n",
        "#Training the Model\n",
        "\n",
        "#Training the model is as easy as passing the input functions that we created earlier.\n",
        "linear_est.train(train_input_function)  # train\n",
        "result = linear_est.evaluate(test_input_function)  # get model metrics/stats by testing on tetsing data\n",
        "\n",
        "clear_output()  # clears consoke output\n",
        "print(result['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOemWvvbNqJ3"
      },
      "source": [
        "check result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y2U9aIXFb0Z"
      },
      "source": [
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x_QxpLGTO8m"
      },
      "source": [
        "** to define probabilty and  logistic**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPEP4ic0Nozf"
      },
      "source": [
        "result=list(linear_est.predict(test_input_function))\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w9f68iQe0Er"
      },
      "source": [
        "if u want to get value of survival and non survival people of titanic we can gwt by ther index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_qcB3X7fE-W"
      },
      "source": [
        "**0 show which is not survived ratio**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgDEJk5fZS3"
      },
      "source": [
        "** 1 index show survival rate **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fZVgBqtgihI"
      },
      "source": [
        "#at 0 noon servival rate\n",
        "print(result[0] )#at zero and 1 both\n",
        "# in output the first aray show 0.93 the non survived rate and 2nd 0.06 is show survived \n",
        "#'probabilities': array([0.9325502 , 0.06744979]\n",
        "\n",
        "#at 1 survival rate\n",
        "print(result[1])\n",
        "#'probabilities': array([0.6364302, 0.3635698"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM-SqODXRpir"
      },
      "source": [
        "#if u want to get just problity from thsi u can use this index\n",
        "print(result[0][\"probabilities\"][0])  #result[0] show survaival rate of 1st person\n",
        "         #[\"probabilities\"][0] zero define is this survied or not bcz [1] is use for survived and [0] is use foe non survied rate\n",
        "# rate which is not survived"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggGloePNfgc6"
      },
      "source": [
        "print(result[1][\"probabilities\"][0]) # 1st person \n",
        "#result[0] show survaival rate of 1st person\n",
        "    #[\"probabilities\"][0] zero define is this survied or not bcz [1] is use for survived and [0] is use foe non survied rate\n",
        "# rate which is not survived"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBYK8Hceh1Nv"
      },
      "source": [
        "prediction walue for **train_data** set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxaFR2dMkJLt"
      },
      "source": [
        "print(result[2][\"probabilities\"][0]) # 2nd person \n",
        "#result[0] show survaival rate of 1st person\n",
        "    #[\"probabilities\"][0] zero define is this survied or not bcz [1] is use for survived and [0] is use foe non survied rate\n",
        "# rate which is not survived"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJvrPJ_bgFBT"
      },
      "source": [
        "print(train_data.loc[0]) #people at index location 0 servival rate and their other value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sMBmgOxiGND"
      },
      "source": [
        "print(train_data.loc[1])  #people at index location 1  servival rate and their other value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVRz4uPsiUUr"
      },
      "source": [
        " print(train_data.loc[2])#people at index location 2  servival rate and their other value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hppNCxX8i7lz"
      },
      "source": [
        "print(train_data.loc[3])#people at index location 3 servival rate and their other value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK3HQjS6k3jk"
      },
      "source": [
        "\n",
        "check person servival and non survial rate by \n",
        "print(train_data.loc[3])\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1phA9OOlQ3h"
      },
      "source": [
        "\n",
        "result of 1 person and sooon.. check by print(result[1])\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M808Lol_liH-"
      },
      "source": [
        "the person is survied or not show by index [0] which show non survived and[1] show survived rate[\"probabilities\"][1]\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XKOQWTijAvw"
      },
      "source": [
        "result=list(linear_est.predict(test_input_function))\n",
        "print(train_data.loc[3])\n",
        "print(result[1][\"probabilities\"][1]) # 1st person \n",
        "#result[0] show survaival rate of 1st person\n",
        "    #[\"probabilities\"][0] zero define is this survied or not bcz [1] is use for survived and [0] is use foe non survied rate\n",
        "# rate which is not survived"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUz_qK4TmFd2"
      },
      "source": [
        "similarly we can use it dfro test aand train data by typr test_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLYpnLcPkV-5"
      },
      "source": [
        "result=list(linear_est.predict(test_input_function))\n",
        "print(test_data.loc[3])\n",
        "print(result[1][\"probabilities\"][1]) # 1st person \n",
        "#result[0] show survaival rate of 1st person\n",
        "    #[\"probabilities\"][0] zero define is this survied or not bcz [1] is use for survived and [0] is use foe non survied rate\n",
        "# rate which is not survived"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZM8Y2svm1Mr"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "now next step is classification\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi0djO0LmqwR"
      },
      "source": [
        "*classification*\n",
        "\n",
        "**Now that we've covered linear regression it is time to talk about classification. Where regression was used to predict a numeric value, classification is used to seperate data points into classes of different labels. In this example we will use a TensorFlow estimator to classify flowers.\n",
        "\n",
        "Since we've touched on how estimators work earlier, I'll go a bit quicker through this example.\n",
        "\n",
        "This section is based on the following guide from the TensorFlow website. https://www.tensorflow.org/tutorials/estimator/premade**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGOCwl5Gd8Hx"
      },
      "source": [
        "**Imports and Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRmvNTRwmlaF"
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPJst7E6eJQG"
      },
      "source": [
        "\n",
        "\n",
        "*   **Dataset**\n",
        "\n",
        "This specific dataset seperates flowers into 3 different classes of species.\n",
        "\n",
        "\n",
        "Setosa\n",
        "\n",
        "Versicolor\n",
        "\n",
        "Virginica\n",
        "\n",
        "The information about each flower is the following.\n",
        "\n",
        "sepal length\n",
        "\n",
        "sepal width\n",
        "\n",
        "petal length\n",
        "\n",
        "petal width\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e75aGiIeEku"
      },
      "source": [
        "flower_information = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
        "species = ['Setosa', 'Versicolor', 'Virginica']\n",
        "# Lets define some constants to help us later on"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_1bBleMejfz"
      },
      "source": [
        "train_path = tf.keras.utils.get_file(\n",
        "    \"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\n",
        "test_path = tf.keras.utils.get_file(\n",
        "    \"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n",
        "\n",
        "train = pd.read_csv(train_path, names=flower_information, header=0)\n",
        "test = pd.read_csv(test_path, names=flower_information, header=0)\n",
        "# Here we use keras (a module inside of TensorFlow) to grab our datasets and read them into a pandas dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxPUb6nsfXSx"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wdwFwzFic3Y"
      },
      "source": [
        "train_flower= train.pop('Species')\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t95SEGWljwK5"
      },
      "source": [
        "test_flower = test.pop('Species')\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRuNxWoJlaHg"
      },
      "source": [
        "train_flower.head() #to get train data which is pop out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4pRXipglr4s"
      },
      "source": [
        "train.shape # there is 120 entries in 4 shapes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VDeUyROl-OT"
      },
      "source": [
        "train_flower.shape # there is 120 entries without 4 shapes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic1IvkJ2mI3L"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiZpOnY0mUvw"
      },
      "source": [
        "**input Function**\n",
        "\n",
        "Remember that nasty input function we created earlier. Well we need to make another one here! Fortunatly for us this one is a little easier to digest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb4OAKwqmZn5"
      },
      "source": [
        "def input_function(features, labels, training=True, batch_size=256):\n",
        "  dataset=tf.data.Dataset.from_tensor_slices((dict(features),labels))\n",
        "  # Convert the inputs to a Dataset.\n",
        "  if training:\n",
        "    dataset=dataset.shuffle(1000).repeat()\n",
        "  return dataset.batch(batch_size)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOJRNo0QpFOy"
      },
      "source": [
        "**Feature Columns**\n",
        "\n",
        "And you didn't think we forgot about the feature columns, did you?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhX1d7XLopqn"
      },
      "source": [
        "#fature colmun have keys which get us value from csv sheet\n",
        "feature_column_get=[]\n",
        "for key in train.keys():\n",
        "  feature_column_get.append(tf.feature_column.numeric_column(key=key))\n",
        "\n",
        "  #tf.feature_column.numeric_column(key=key)  this is use in input function to get key value from function\n",
        "print(feature_column_get)  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f5a0dBiq5Np"
      },
      "source": [
        "**whole code here  which have create model and input function and feature colum in input finction**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXbFjZJFpTrs"
      },
      "source": [
        "flower_information = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
        "species = ['Setosa', 'Versicolor', 'Virginica']\n",
        "# Lets define some constants to help us later on\n",
        "train_path = tf.keras.utils.get_file(\n",
        "    \"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\n",
        "test_path = tf.keras.utils.get_file(\n",
        "    \"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n",
        "\n",
        "train = pd.read_csv(train_path, names=flower_information, header=0)\n",
        "test = pd.read_csv(test_path, names=flower_information, header=0)\n",
        "# Here we use keras (a module inside of TensorFlow) to grab our datasets and read them into a pandas dataframe\n",
        "def input_fn(features, labels, training=True, batch_size=256):\n",
        "  dataset=tf.data.Dataset.from_tensor_slices((dict(features),labels))\n",
        "  # Convert the inputs to a Dataset.\n",
        "  if training:\n",
        "    dataset=dataset.shuffle(1000).repeat()\n",
        "  return dataset.batch(batch_size)  \n",
        "feature_column_get=[]\n",
        "for key in train.keys():\n",
        "  feature_column_get.append(tf.feature_column.numeric_column(key=key))\n",
        "\n",
        "  #tf.feature_column.numeric_column(key=key)  this is use in input function to get key value from function\n",
        "print(feature_column_get)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fHIlPYnis2P"
      },
      "source": [
        "###Building the Model\n",
        "And now we are ready to choose a model. For classification tasks there are variety of different estimators/models that we can pick from. Some options are listed below.\n",
        "- ```DNNClassifier``` (Deep Neural Network)\n",
        "- ```LinearClassifier```\n",
        "\n",
        "We can choose either model but the DNN seems to be the best choice. This is because we may not be able to find a linear coorespondence in our data. \n",
        "\n",
        "So let's build a model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01sC65eJjQas"
      },
      "source": [
        "---\n",
        "**LinearClassifier is very similar to linear  regression**\n",
        "*the probablity being have spcific label rather then a numeric value*\n",
        "\n",
        "\n",
        "\n",
        "its oour choice what we chose we work with DNN \n",
        "\n",
        "but \n",
        "\n",
        "tepically we can use LINEAR CLAASSIFIER FOR MACHINE LEARNING APPS AND USE FOR LOADING AND PRE PROCESSING OF Data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**LinearClassifier is very similar to linear  regression**\n",
        "*the probablity being have spcific label rather then a numeric value*\n",
        "\n",
        "\n",
        "\n",
        "its oour choice what we chose we work with DNN \n",
        "\n",
        "but \n",
        "\n",
        "tepically we can use LINEAR CLAASSIFIER FOR MACHINE LEARNING APPS AND USE FOR LOADING AND PRE PROCESSING OF Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkPWr9xRrKXA"
      },
      "source": [
        "#estimator have bunch of pre made  models of tensorflow in it\n",
        "# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.\n",
        "classifier = tf.estimator.DNNClassifier(\n",
        "    feature_columns=feature_column_get,\n",
        "    # Two hidden layers of 30 and 10 nodes respectively.   (middle ,input and out-put ther are 3 layers...)\n",
        "    hidden_units=[30, 10],# hidden_units are the core architucture of dnn\n",
        "    #30 node in first layer and 10 are in second layer\n",
        "    # The model must choose between 3 classes.\n",
        "    n_classes=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KILjL698mzXu"
      },
      "source": [
        "What we've just done is created a deep neural network that has two hidden layers. These layers have 30 and 10 neurons respectively. This is the number of neurons the TensorFlow official tutorial uses so we'll stick with it. However, it is worth mentioning that the number of hidden neurons is an arbitrary number and many experiments and tests are usually done to determine the best choice for these values. Try playing around with the number of hidden neurons and see if your results change.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsPQ0XPvnJIB"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Training**\n",
        "\n",
        "Now it's time to train the model!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlQhNEGAu5BG"
      },
      "source": [
        "#classifier.train(). is use for train data loss classifiction\n",
        "#tensorflow:Loss for final step: 0.18689048\n",
        "\n",
        "classifier.train(\n",
        "    input_fn=lambda: input_fn(train, train_flower, training=True),\n",
        "    steps=5000)   #lamda is an anonmays that can be define that this is a one-line function and show this is a function and \n",
        "    #input_fn(train, train_y, training=True).  this show what does function does ???\n",
        "    #steps=5000 is equal to---> epoch=5000\n",
        "#example of lamba function. \n",
        "\n",
        "#x=lambda: print(\"Hammad\") --> show one line function  this is a function\n",
        "#x()\n",
        "# ------> it clearly show this is a function which we want to get -->input_fn(train, train_y, training=True) \n",
        "# We include a lambda to avoid creating an inner function previously"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR_v_N-Au_ye"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "---\n",
        "tensorflow:Loss for final step: 0.18689048 ----> show loss in our train model\n",
        "\n",
        "---\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxvltgl6vK77"
      },
      "source": [
        "\n",
        "---\n",
        "simiilarly it use for our test model...> which show his loss lets see\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD1lGS_Ju5w7"
      },
      "source": [
        "#classifier.test(). is use for  Test set accuracy: 0.967\n",
        "\n",
        "eval_result = classifier.evaluate(\n",
        "    input_fn=lambda: input_fn(test, test_flower, training=False))\n",
        "# training=False -----> bcz we test our test flower data\n",
        "\n",
        "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHNWcB-ywTN-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}